{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MEiNkkGEmhT"
      },
      "source": [
        "## **Case Studies in Data Analytics Assignment 2**\n",
        "\n",
        "Group Number: 1\n",
        "\n",
        "Group Member Names:\n",
        "\n",
        "1.   Shimita Rudra (22221035)\n",
        "2.   Ayusha Kakkad (22210321)\n",
        "3.   Sanju Kumari(22221634)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyD73xODjJjQ"
      },
      "source": [
        "### Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNbWbhuFO8om"
      },
      "outputs": [],
      "source": [
        "# Mounting the dataset from google drive in colab.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAHfjMUlELMs"
      },
      "source": [
        "### Import Packages and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJZfpCl0PDLM"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import random\n",
        "%reload_ext tensorboard\n",
        "import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9iQndusPGGY"
      },
      "outputs": [],
      "source": [
        "# read the image folder path from goole drive\n",
        "\n",
        "image_folder = '/content/drive/MyDrive/dataset/'\n",
        "\n",
        "# set lables for prediction\n",
        "lables = [\"not alpaca\", \"alpaca\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cnF7EdKjv4m"
      },
      "source": [
        "### Proposed Layer Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYZ2ROGnkBY6"
      },
      "source": [
        "#### Read Images and Convert to GreyScale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWch95EEPQ7Y"
      },
      "outputs": [],
      "source": [
        "# Read all the images from folder\n",
        "\n",
        "image_dataset = []\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for idx, i in enumerate(lables):\n",
        "  path = os.path.join(image_folder,i)\n",
        "  label_num = idx\n",
        "  for image in os.listdir(path):\n",
        "    #Convert to greyscale\n",
        "    image_array = cv2.imread(os.path.join(path, image), cv2.IMREAD_GRAYSCALE)\n",
        "    try:\n",
        "      image_array = cv2.resize(image_array,(32,32),interpolation=cv2.INTER_AREA)\n",
        "    except:\n",
        "      break\n",
        "    image_dataset.append([image_array,label_num])\n",
        "\n",
        "for i, j in image_dataset:\n",
        "  X.append(i)\n",
        "  y.append(j)\n",
        "y = np.array(y)\n",
        "X=np.array(X).reshape(-1,32,32,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKpo_E3QRXwA"
      },
      "source": [
        "#### Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUy7WSqXQISP"
      },
      "outputs": [],
      "source": [
        "X_Train, X_Test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=12) # 70% data for training\n",
        "X_Val, X_Test, y_val, y_test= train_test_split(X_Test, y_test, test_size=0.5, random_state=12) # 15% data for test and validation\n",
        "X_Train = tf.cast(X_Train, tf.float32)\n",
        "X_Val = tf.cast(X_Val, tf.float32)\n",
        "X_Test = tf.cast(X_Test, tf.float32)\n",
        "X_Train.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3OkOks5J-tw"
      },
      "source": [
        "#### Proposed Layer Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJmkxrFOQV6_"
      },
      "outputs": [],
      "source": [
        "class ProposedLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, depth_weight_matrics, kernel_size, **kwargs):\n",
        "    self.depth_weight_matrics = depth_weight_matrics\n",
        "    self.kernel_size = kernel_size\n",
        "    super(ProposedLayer, self).__init__(**kwargs)\n",
        "\n",
        "#Build Function to initialize weight matrix\n",
        "  def build(self, input_shape):\n",
        "    self.weight_matrix = self.add_weight(name='kernel',shape=(input_shape[1], input_shape[2], input_shape[-1], self.depth_weight_matrics),\n",
        "                                  initializer='glorot_uniform',trainable=True)\n",
        "    super(ProposedLayer, self).build(input_shape)\n",
        "\n",
        "#Call Function\n",
        "  def call(self, input):\n",
        "    outputs = []\n",
        "    #print(\"Input matrix\", input[:,:,:])\n",
        "    for k in range(0,self.depth_weight_matrics):\n",
        "      #print(self.weight_matrix[:,:,0,k])\n",
        "      #print(\"input :\", input[:,:,:,0])\n",
        "#Matrix Multiplication of Input and Weight Matrix\n",
        "      x = tf.math.multiply(input[:,:,:,0], self.weight_matrix[:,:,0,k])\n",
        "      x = tf.expand_dims(x,axis=-1)\n",
        "      #print(\"x:\",x.shape)\n",
        "      #print(input,\"\\n\", type(x),\"\\n\", type(input))\n",
        "      receptive_field = tf.cast(tf.constant(np.ones((3,3,1,1))), tf.float32)\n",
        "      #print(receptive_field)\n",
        "      #print(\"WEight mat :\",type(self.weight_matrix),self.weight_matrix, \"\\n\" ,self.weight_matrix[:,:,0,k])\n",
        "      #print(\"X:\", type(x),x.shape)\n",
        "\n",
        "#tf.nn.conv2D function\n",
        "      output = tf.nn.conv2d(x, receptive_field, strides = 1, padding=\"SAME\")\n",
        "      outputs.append(output)\n",
        "      #print(\"output :\", output.shape)\n",
        "    outputs = tf.concat(outputs, axis=-1)\n",
        "    #print(\"output shape\",output.shape)\n",
        "    return outputs\n",
        "\n",
        "#Compute the gradients of the layer's\n",
        "  def gradient(self, x, gradients):\n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch()\n",
        "      outputs = self.call(x)\n",
        "    output_gradients = tape.gradient(outputs, x, gradients)\n",
        "    weight_gradients = tape.gradient(outputs, self.weight_matrix, gradients)\n",
        "    print(weight_gradients)\n",
        "    return output_gradients, weight_gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxJYT6yWEHxX"
      },
      "source": [
        "#### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK27vM4sEHZb"
      },
      "outputs": [],
      "source": [
        "#Augment data with RandomFlip,RandomRotation and RandomZoom of Images\n",
        "custom_ccn_model = models.Sequential( [\n",
        "    layers.RandomFlip(\"horizontal\",\n",
        "                      input_shape=(32,\n",
        "                                  32,\n",
        "                                  1)),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "  ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-48PoH9L1xB"
      },
      "source": [
        "#### Create Sequential Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ry6IxGcrQxnX"
      },
      "outputs": [],
      "source": [
        "#Preprocessed Images put in Input Layer\n",
        "custom_ccn_model.add(layers.InputLayer())\n",
        "\n",
        "#Kernal Number = 16,Weighted Matrix Size = (3,3)\n",
        "custom_ccn_model.add(ProposedLayer(depth_weight_matrics=16,kernel_size=3))\n",
        "\n",
        "#Apply Relu Activation Function\n",
        "custom_ccn_model.add(layers.Activation('relu'))\n",
        "\n",
        "#Max Pooling Kernel Size = (2,2)\n",
        "custom_ccn_model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "#Kernal Number = 12,Weighted Matrix Size = (3,3)\n",
        "custom_ccn_model.add(ProposedLayer(depth_weight_matrics=12,kernel_size=3))\n",
        "\n",
        "#Apply Relu Activation Function\n",
        "custom_ccn_model.add(layers.Activation('relu'))\n",
        "\n",
        "#Max Pooling Kernel Size = (2,2)\n",
        "custom_ccn_model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "#Kernal Number = 16,Weighted Matrix Size = (3,3)\n",
        "custom_ccn_model.add(ProposedLayer(depth_weight_matrics=8,kernel_size=3))\n",
        "\n",
        "#Apply Relu Activation Function\n",
        "custom_ccn_model.add(layers.Activation('relu'))\n",
        "\n",
        "#Convert to 1 Dimensional\n",
        "custom_ccn_model.add(layers.Flatten())\n",
        "\n",
        "#Performing Linear Operation on input\n",
        "custom_ccn_model.add(layers.Dense(256))\n",
        "\n",
        "#Apply Relu Activation Function\n",
        "custom_ccn_model.add(layers.Activation('relu'))\n",
        "\n",
        "#Applying softmax activation to output result\n",
        "custom_ccn_model.add(layers.Dense(1, activation = 'softmax'))\n",
        "custom_ccn_model(X_Train)\n",
        "\n",
        "#Print Model Summary\n",
        "custom_ccn_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRhH0T8ydD8h"
      },
      "source": [
        "#### Proposed Layer Model Compilation and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4Vs4-bzfWjl"
      },
      "outputs": [],
      "source": [
        "#Stochastic Gradient Optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "#Compile Model Trained\n",
        "custom_ccn_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['mse', 'accuracy', tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
        "\n",
        "#Tensorboard callbacks\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback1 = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "#60 epochs\n",
        "epo = 60\n",
        "\n",
        "#Train the model\n",
        "hist=custom_ccn_model.fit(X_Train, y_train,\n",
        "                    epochs=epo, validation_data=(X_Val, y_val),batch_size = 32,callbacks=[tensorboard_callback1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT4eZuxDlCD_"
      },
      "source": [
        "#### Proposed Layer Model Evaluation and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91Go97uy2HV8"
      },
      "outputs": [],
      "source": [
        "#Evaluation on binary_crossentropy loss with mse,accuracy,precision,recall metrics\n",
        "test_loss, test_mse, test_acc,  test_precision, test_recall = custom_ccn_model.evaluate(X_Test, y_test)\n",
        "\n",
        "#Image Prediction\n",
        "y_pred_new = custom_ccn_model.predict(X_Test)\n",
        "y_pred_new = np.around(y_pred_new)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9rkCV-BloJm"
      },
      "source": [
        "#### Training and Validation Loss Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNz9WHxm2Y9E"
      },
      "outputs": [],
      "source": [
        "#X axis = Epochs\n",
        "#Y axis = Loss value\n",
        "acc = hist.history['accuracy']\n",
        "val_acc = hist.history['val_accuracy']\n",
        "\n",
        "loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "\n",
        "epochs_range = range(epo)\n",
        "\"\"\"\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\"\"\"\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss with learning rate 0.01,Epochs size 60 and batch size 32')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGtmiivFRgzK"
      },
      "source": [
        "#### Tensorboard Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xmPUJwNfb2L"
      },
      "outputs": [],
      "source": [
        "  !tensorboard dev upload --logdir ./logs \\\n",
        "  --name \"Proposed Layer Model Implementation with SGD Optimizer\" \\\n",
        "  --description \"Training results from https://colab.sandbox.google.com/github/tensorflow/tensorboard/blob/master/docs/tbdev_getting_started.ipynb\" \\\n",
        "  --one_shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6KV3KrsQt5y"
      },
      "source": [
        "### Proposed Layer Model Implementation with Different Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9kcevkmPvqb"
      },
      "outputs": [],
      "source": [
        "#Learning rates - 0.01,0.001,0.05\n",
        "#epochs(epo) - 200,400,50\n",
        "#batch sizes - 8,16,32\n",
        "#optimizer - adam\n",
        "\n",
        "from matplotlib.ticker import NullFormatter\n",
        "for i in (0.01,0.001,0.05):\n",
        "  batch_sizes = ''\n",
        "  epo = ''\n",
        "  if i == 0.01:\n",
        "    batch_sizes=8\n",
        "    epo=200\n",
        "    if i==0.001:\n",
        "      batch_sizes=16\n",
        "      epo=400\n",
        "  else:\n",
        "    batch_sizes=32\n",
        "    epo=50\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=i)\n",
        "  custom_ccn_model.compile(optimizer=optimizer, loss='binary_crossentropy',\n",
        "                           metrics=['mse', 'accuracy', tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
        "  log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback1 = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "  hist=custom_ccn_model.fit(X_Train, y_train,\n",
        "                    epochs=epo, validation_data=(X_Val, y_val), batch_size = batch_sizes,callbacks=[tensorboard_callback1])\n",
        "  test_loss, test_mse, test_acc,  test_precision, test_recall = custom_ccn_model.evaluate(X_Test, y_test)\n",
        "\n",
        "  y_pred_new = custom_ccn_model.predict(X_Test)\n",
        "  y_pred_new = np.around(y_pred_new)\n",
        "\n",
        "  acc = hist.history['accuracy']\n",
        "  val_acc = hist.history['val_accuracy']\n",
        "\n",
        "  loss = hist.history['loss']\n",
        "  val_loss = hist.history['val_loss']\n",
        "\n",
        "  epochs_range = range(epo)\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(epochs_range, loss, label='Training Loss')\n",
        "  plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.title(\"Training and Validation Loss with learning rate {} ,batch size {} and epoch size {}\".format(i,batch_sizes,epo))\n",
        "  #plt.title('Training and Validation Loss with learning rate :',i, )\n",
        "  plt.show()\n",
        "  !tensorboard dev upload --logdir ./logs \\\n",
        "  --name \"Proposed Layer Model Implementation with Different Hyperparameters with Optimizer Adam\" \\\n",
        "  --description \"Training results from https://colab.sandbox.google.com/github/tensorflow/tensorboard/blob/master/docs/tbdev_getting_started.ipynb\" \\\n",
        "  --one_shot\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZJ3fH4EQHvz"
      },
      "source": [
        "### Traditional Convolution Neural Network without Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REWHT8sgpLj2"
      },
      "source": [
        "#### Import Packages and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPAWp6124j0y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, InputLayer, Input, Dropout, RandomFlip, RandomRotation,Rescaling\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6EPOIASqKFM"
      },
      "source": [
        "#### Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTnYtWSFqGKj"
      },
      "outputs": [],
      "source": [
        "Batch_size = 32\n",
        "Image_height=32\n",
        "Image_width=32\n",
        "\n",
        "train_dataset = image_dataset_from_directory(image_folder,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=Batch_size,\n",
        "                                             color_mode='grayscale',\n",
        "                                             image_size=(Image_height,Image_width),\n",
        "                                             validation_split=0.3,\n",
        "                                             subset='training',\n",
        "                                             seed=30)\n",
        "val_dataset = image_dataset_from_directory(image_folder,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=Batch_size,\n",
        "                                             color_mode='grayscale',\n",
        "                                             image_size=(Image_height,Image_width),\n",
        "                                             validation_split=0.3,\n",
        "                                             subset='validation',\n",
        "                                             seed=30)\n",
        "\n",
        "class_names = train_dataset.class_names\n",
        "\n",
        "num_classes = len(class_names)\n",
        "\n",
        "for image_batch, labels_batch in train_dataset:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14nVt-qPrLLU"
      },
      "source": [
        "#### Create Sequential Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH98lL6r5BY6"
      },
      "outputs": [],
      "source": [
        "from keras.metrics.confusion_metrics import Precision\n",
        "\n",
        "#Autotune to efficiently load data\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_dataset = train_dataset.cache().shuffle(100).prefetch(buffer_size=AUTOTUNE)\n",
        "val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "ccn_model_1 = Sequential([\n",
        "  #rescale input\n",
        "  layers.Rescaling(1./255, input_shape=(32, 32, 1)),\n",
        "\n",
        "  #Apply Relu Activation Function\n",
        "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "\n",
        "  #Max Pooling Kernel Size = (2,2)\n",
        "  layers.MaxPooling2D(),\n",
        "\n",
        "  #Apply Relu Activation Function\n",
        "  layers.Conv2D(12, 3, padding='same', activation='relu'),\n",
        "\n",
        "  #Max Pooling Kernel Size = (2,2)\n",
        "  layers.MaxPooling2D(),\n",
        "\n",
        "  #Apply Relu Activation Function\n",
        "  layers.Conv2D(8, 3, padding='same', activation='relu'),\n",
        "\n",
        "  #Max Pooling Kernel Size = (2,2)\n",
        "  layers.MaxPooling2D(),\n",
        "\n",
        "  #Convert to 1 Dimensional\n",
        "  layers.Flatten(),\n",
        "\n",
        "  layers.Dense(256, activation='relu'),\n",
        "\n",
        "  #Applying softmax activation to output result\n",
        "  layers.Dense(num_classes, activation='softmax')\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQLrk_BPrbFp"
      },
      "source": [
        "#### Traditional CNN Model Compilation and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ubAI-qlrasZ"
      },
      "outputs": [],
      "source": [
        "#Compile model\n",
        "ccn_model_1.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#create model summary\n",
        "ccn_model_1.summary()\n",
        "\n",
        "#Tensorboard Callbacks\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback2 = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "epochs=10\n",
        "\n",
        "#Train model\n",
        "ccn_model_1_hist = ccn_model_1.fit(\n",
        "  train_dataset,\n",
        "  validation_data=val_dataset,\n",
        "  epochs=epochs,\n",
        "  callbacks=[tensorboard_callback2]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux0PgbjsueKN"
      },
      "source": [
        "#### Traditional CNN Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX9jwMyOudaI"
      },
      "outputs": [],
      "source": [
        "y_pred_new = ccn_model_1.predict(X_Test)\n",
        "y_pred_new = np.around(y_pred_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X4S2pAes-F0"
      },
      "source": [
        "#### Tensorboard Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhgiBELG6NZj"
      },
      "outputs": [],
      "source": [
        "#%tensorboard --logdir logs/fit\n",
        "\n",
        "!tensorboard dev upload --logdir ./logs \\\n",
        "  --name \"Traditional_CNN_without_DataAugmentation\" \\\n",
        "  --description \"Training results from https://colab.sandbox.google.com/github/tensorflow/tensorboard/blob/master/docs/tbdev_getting_started.ipynb\" \\\n",
        "  --one_shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LF5G96huKxa"
      },
      "source": [
        "#### Training,Validation Accuracy and Loss Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVcpm9QV6R9U"
      },
      "outputs": [],
      "source": [
        "#X axis = Epochs\n",
        "#Y axis = Loss, Accuracy value\n",
        "\n",
        "acc = ccn_model_1_hist.history['accuracy']\n",
        "val_acc = ccn_model_1_hist.history['val_accuracy']\n",
        "\n",
        "loss = ccn_model_1_hist.history['loss']\n",
        "val_loss = ccn_model_1_hist.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT_K4iornjDG"
      },
      "source": [
        "#### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTIiYc1vnfib"
      },
      "outputs": [],
      "source": [
        "#Augment data with RandomFlip,RandomRotation and RandomZoom of Images\n",
        "data_augmentation = keras.Sequential(\n",
        "  [\n",
        "    layers.RandomFlip(\"horizontal\",\n",
        "                      input_shape=(32,\n",
        "                                  32,\n",
        "                                  1)),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwYAoN5ZQPc4"
      },
      "source": [
        "### Traditional CNN Model with Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nbf733N6xZW"
      },
      "outputs": [],
      "source": [
        "\n",
        "ccn_model_2 = Sequential([\n",
        "  data_augmentation,\n",
        "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(12, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(8, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(256, activation='relu'),\n",
        "  layers.Dense(num_classes, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9_jJmDvxDtX"
      },
      "source": [
        "#### Model Compilation and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_CtHBm2xDTf"
      },
      "outputs": [],
      "source": [
        "ccn_model_2.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "ccn_model_2.summary()\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback3 = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "epochs = 10\n",
        "ccn_model_2_hist = ccn_model_2.fit(\n",
        "  train_dataset,\n",
        "  validation_data=val_dataset,\n",
        "  epochs=epochs,\n",
        "  callbacks=[tensorboard_callback3]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbZxyslEhAxn"
      },
      "source": [
        "####Tensorboard Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlh6Y3vnYunX"
      },
      "outputs": [],
      "source": [
        "!tensorboard dev upload --logdir ./logs \\\n",
        "  --name \"Traditional_CNN_with_DataAugmentation\" \\\n",
        "  --description \"Training results from https://colab.sandbox.google.com/github/tensorflow/tensorboard/blob/master/docs/tbdev_getting_started.ipynb\" \\\n",
        "  --one_shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KlprAm0xAZp"
      },
      "source": [
        "#### Training,Validation Accuracy and Loss Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8Zsoqr667of"
      },
      "outputs": [],
      "source": [
        "acc2 = ccn_model_2_hist.history['accuracy']\n",
        "val_acc2 = ccn_model_2_hist.history['val_accuracy']\n",
        "\n",
        "loss2 = ccn_model_2_hist.history['loss']\n",
        "val_loss2 = ccn_model_2_hist.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37f5RAvn6_Ki"
      },
      "outputs": [],
      "source": [
        "#X axis = Epochs\n",
        "#Y axis = Loss,Accuracy value\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc2, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc2, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss2, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss2, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpKBHLUHPqjO"
      },
      "source": [
        "### Proposed Layer Model for 1-Dimensional Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zraUCkHeDPhq"
      },
      "outputs": [],
      "source": [
        "class ProposedLayer_1D(tf.keras.layers.Layer):\n",
        "  def __init__(self, depth_weight_matrics, kernel_size, **kwargs):\n",
        "    self.depth_weight_matrics = depth_weight_matrics\n",
        "    self.kernel_size = kernel_size\n",
        "    super(ProposedLayer_1D, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    #adjusted kernal size\n",
        "    self.weight_matrix = self.add_weight(name='kernel', shape=(input_shape[-1], self.depth_weight_matrics),\n",
        "                                  initializer='glorot_uniform', trainable=True)\n",
        "    super(ProposedLayer_1D, self).build(input_shape)\n",
        "\n",
        "  def call(self, input):\n",
        "    outputs = []\n",
        "    for k in range(0, self.depth_weight_matrics):\n",
        "      x = tf.math.multiply(input[:, :, 0], self.weight_matrix[:, k])\n",
        "      #convert to NLC format\n",
        "      x = tf.expand_dims(x, axis=-1)\n",
        "      receptive_field = tf.cast(tf.constant(np.ones((1, 1, 1, 1))), tf.float32)\n",
        "      output = tf.nn.conv2d(x, receptive_field, strides=1, padding=\"SAME\")\n",
        "      outputs.append(output)\n",
        "    outputs = tf.concat(outputs, axis=3)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe1IAVH8P0Dr"
      },
      "source": [
        "### Proposed Layer Model for 3-Dimensional Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdGZk5nZD_Mp"
      },
      "outputs": [],
      "source": [
        "class ProposedLayer_3D(tf.keras.layers.Layer):\n",
        "  #Adjusted kernel size\n",
        "  def __init__(self, depth_weight_matrices, kernel_size=(3,3,3), **kwargs):\n",
        "    self.depth_weight_matrices = depth_weight_matrices\n",
        "    super(ProposedLayer_3D, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.weight_matrix = self.add_weight(name='kernel', shape=(self.kernel_size[0], self.kernel_size[1], self.kernel_size[2], input_shape[-1], self.depth_weight_matrices),\n",
        "                                  initializer='glorot_uniform', trainable=True)\n",
        "    super(ProposedLayer_3D, self).build(input_shape)\n",
        "\n",
        "  def call(self, input):\n",
        "    outputs = []\n",
        "    for k in range(self.depth_weight_matrices):\n",
        "      x = tf.math.multiply(input[:,:,:,0,:], self.weight_matrix[:,:,0,:,k])\n",
        "      x = tf.expand_dims(x, axis=-1)\n",
        "\n",
        "      #Converting NHWDC format\n",
        "      receptive_field = tf.cast(tf.constant(np.ones((3,3,3,1,1))), tf.float32)\n",
        "      output = tf.nn.conv3d(x, receptive_field, strides=1, padding=\"SAME\")\n",
        "      outputs.append(output)\n",
        "    outputs = tf.concat(outputs, axis=-1)\n",
        "    return outputs\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}