{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su3NgQQDBZMq"
      },
      "source": [
        "Name: Ayusha Kakkad\n",
        "\n",
        "Student ID: 22210321"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task1 : Generate Top View"
      ],
      "metadata": {
        "id": "U1fsxBva7Yb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For TopView Generation following steps were followed which will be explained in detail during the code implementation:\n",
        "\n",
        "1) Loading of Images in the Dataset (Mono-Left, Mono-Right, Mono-Rear,Stereo-Centre,Stereo-Right,Stereo-Left)\n",
        "\n",
        "2) Convert the Grey Images in Dataset to RGB\n",
        "\n",
        "3) Demosaic all the Images\n",
        "\n",
        "4) For all FishEye Calibrated Images Undistortion is performed\n",
        "\n",
        "5) WrapPerspective Transform Function is applied on Mono-Left,Stereo Centre,Mono-Right and Mono-Rear\n",
        "\n",
        "6) Images are Rotated to generate an appropriate Top-View.\n",
        "\n",
        "7) Top-View is generated by Stitching the 4 set of images rotated in above step"
      ],
      "metadata": {
        "id": "esdTIx7I5Y8L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWP-PJLFFH5m"
      },
      "source": [
        "### Package Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEwMwv4XNSSP"
      },
      "outputs": [],
      "source": [
        "# Installing the colour_demosaicing function required for demosaic function\n",
        "!pip install colour_demosaicing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykJbTS1wViob"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from PIL import Image\n",
        "from colour_demosaicing import demosaicing_CFA_Bayer_bilinear as demosaic\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting Google Drive"
      ],
      "metadata": {
        "id": "hj_Q5tcR-gIv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmFA_Wl8dJwF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwWSsTDAFWaj"
      },
      "source": [
        "### RGB + Demosiac Function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demosaic Function reconstructs a grey image provided in the dataset to a full-resolution color image from the sampled data acquired by a digital camera model. The digital camera model applies a color filter array to a single sensor.\n",
        "\n",
        "If the image is part of Stereo images then images are only demosaiced. For mono images undistortion is also performed."
      ],
      "metadata": {
        "id": "w8TMmUBz9J9f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unhihXz8VVz9"
      },
      "outputs": [],
      "source": [
        "BAYER_STEREO = 'gbrg'\n",
        "BAYER_MONO = 'rggb'\n",
        "\n",
        "#image_path = provides path to images stored in database\n",
        "def load_image(image_path, model=None, debayer=True):\n",
        "\n",
        "    if model:\n",
        "        camera = model.camera\n",
        "    else:\n",
        "        camera = re.search('(stereo|mono_(left|right|rear))', image_path).group(0)\n",
        "#if stereo demosaic\n",
        "    if camera == 'stereo':\n",
        "        pattern = BAYER_STEREO\n",
        "    else:\n",
        "        pattern = BAYER_MONO\n",
        "\n",
        "    img = Image.open(image_path)\n",
        "    if debayer:\n",
        "        img = demosaic(img, pattern)\n",
        "#if mono undistort which is implemented in future code\n",
        "    if camera=='mono':\n",
        "        img = model.undistort(img)\n",
        "\n",
        "    return np.array(img).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Images"
      ],
      "metadata": {
        "id": "9ozH-j3xCGb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Drive is Mounted and the Dataset consisting of Mono-Right,Mono Left,Mono-Rear, Stereo-Centre,Stereo-Right,Stereo-Left folders are loaded which consist of 40 images each.\n",
        "\n",
        "Originally there were 49 images in the stereo folders but for implementation the last 9 images have been removed.\n",
        "\n",
        "With the help of matplotlib.pyplot as plt I have plot all images are plot below. enumerate is used for providing the count of images.str(img+1) provides the number of images as they print."
      ],
      "metadata": {
        "id": "zkQyfeOnPMIu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDK-4gu5TKav"
      },
      "outputs": [],
      "source": [
        "def load_images_from_folder(input_path):\n",
        "    filenames = [input_path + filename for filename in os.listdir(input_path)]\n",
        "    images = []\n",
        "    for filename in filenames:\n",
        "            image = load_image(filename)\n",
        "            images.append(image)\n",
        "    return images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eW5WJuPFicX"
      },
      "source": [
        "####Mono Left Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKsP5hOgZst7"
      },
      "outputs": [],
      "source": [
        "mono_left_images = load_images_from_folder('/content/drive/MyDrive/sample_small/mono_left/')\n",
        "for img, image in enumerate(mono_left_images):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.title(str(\"Mono Left Images \"+ str(img+1)))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8bjpGoVFlzw"
      },
      "source": [
        "####Mono Right Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRvXsB3ihcVk"
      },
      "outputs": [],
      "source": [
        "mono_right_images = load_images_from_folder('/content/drive/MyDrive/sample_small/mono_right/')\n",
        "for img, image in enumerate(mono_right_images):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.title(str(\"Mono Right Images \"+ str(img+1)))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvkNE3VnFoUs"
      },
      "source": [
        "#### Mono Rear Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q08IfGdhgtS"
      },
      "outputs": [],
      "source": [
        "mono_rear_images= load_images_from_folder('/content/drive/MyDrive/sample_small/mono_rear/')\n",
        "for img, image in enumerate(mono_rear_images):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.title(str(\"Mono Rear Images \"+ str(img+1)))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR7JOjHfFrBB"
      },
      "source": [
        "#### Stereo Centre Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o27OtANVhnGJ"
      },
      "outputs": [],
      "source": [
        "stereo_centre_images = load_images_from_folder('/content/drive/MyDrive/sample_small/stereo/centre/')\n",
        "for img, image in enumerate(stereo_centre_images):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.title(str(\"Stereo Centre Images \"+ str(img+1)))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ndIJRTBFylr"
      },
      "source": [
        "#### Stereo Left Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOAIdn8Yho0h"
      },
      "outputs": [],
      "source": [
        "stereo_left_images =load_images_from_folder('/content/drive/MyDrive/sample_small/stereo/left/')\n",
        "for img, image in enumerate(stereo_left_images):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.title(str(\"Stereo Left Images \"+ str(img+1)))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxKfO2ZgF4aU"
      },
      "source": [
        "####Stereo Right Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogv0BVn0hqhl"
      },
      "outputs": [],
      "source": [
        "stereo_right_images=load_images_from_folder('/content/drive/MyDrive/sample_small/stereo/right/')\n",
        "for img, image in enumerate(stereo_right_images):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.title(str(\"Stereo Right Images \"+ str(img+1)))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Improving Colour Shade of Stereo Centre Image"
      ],
      "metadata": {
        "id": "LPCKsBzINF92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stereo Images have a blusish shade of roads post demosaicing the images. For generating accurate Top View, the colour shade of Stereo Centre(Front) Images are improved by Histogram Matching.\n",
        "\n",
        "Histogram matching can be used as a lightweight normalisation for image processing, such as feature matching,where the images have been taken in different conditions. The stereo centre images are taken and matched with mono rear images colour shade and histogram."
      ],
      "metadata": {
        "id": "pmNhdixmVq8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.exposure import cumulative_distribution\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "def hist_matching(c, c_t, im):\n",
        "    b = np.interp(c, c_t, np.arange(256))   # find closest matches to b_t\n",
        "    pix_repl = {i:b[i] for i in range(256)} # dictionary to replace the pixels\n",
        "    mp = np.arange(0,256)\n",
        "    for (k, v) in pix_repl.items():\n",
        "        mp[k] = v\n",
        "    s = im.shape\n",
        "    im = np.reshape(mp[im.ravel()], im.shape)\n",
        "    im = np.reshape(im, s)\n",
        "    return im\n",
        "\n",
        "def cdf(im):\n",
        "    c, b = cumulative_distribution(im)\n",
        "    for i in range(b[0]):\n",
        "        c = np.insert(c, 0, 0)\n",
        "    for i in range(b[-1]+1, 256):\n",
        "        c = np.append(c, 1)\n",
        "    return c\n",
        "\n",
        "# Load stereo and mono_rear images\n",
        "img2 = mono_rear_images[0]\n",
        "for i in range(len(stereo_centre_images)):\n",
        "    img1 = stereo_centre_images[i]\n",
        "\n",
        "    # Match colors\n",
        "    im1 = np.zeros(img1.shape).astype(np.uint8)\n",
        "    for j in range(3):\n",
        "        c = cdf(img1[...,j])\n",
        "        c_t = cdf(img2[...,j])\n",
        "        im1[...,j] = hist_matching(c, c_t, img1[...,j])\n",
        "    stereo_centre_images[i] = im1\n",
        "\n",
        "#Plot Images\n",
        "for img, image in enumerate(stereo_centre_images):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.title(str(\"Stereo Centre Improvised Images \"+ str(img+1)))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "idt_2W0nK60_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A_k2gOiF_rc"
      },
      "source": [
        "###Undistorting Images"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Undistorting the fish eye calibrated mono images, first we obtain the model name (stereo left|centre|right or mono left|right|rear),next we obtain the image directory with model name + from  models folder which ends with _distortion_lut.bin file. After getting both,the undistortion function is applied.\n",
        "\n",
        "There is a check for \"Incorrect image size for camera model\" and whether the image being undistorted is an multi-channel image.\n",
        "\n",
        "Image distortion is when the straight lines of an image appear to be deformed or curved unnaturally. Distortion can significantly disrupt the image's quality. Undistorting the image will result in better quality of the image and straight lines in the image."
      ],
      "metadata": {
        "id": "pfE2fKwHbEMM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mvStC022C-a"
      },
      "outputs": [],
      "source": [
        "#get image\n",
        "def load_lut(models_dir, images_dir):\n",
        "  model_name = __get_model_name(images_dir)\n",
        "  lut_path = os.path.join(models_dir, model_name + '_distortion_lut.bin')\n",
        "  lut = np.fromfile(lut_path, np.double)\n",
        "  lut = lut.reshape([2, lut.size // 2])\n",
        "  bilinear_lut = lut.transpose()\n",
        "  return bilinear_lut\n",
        "\n",
        "#get model name\n",
        "def __get_model_name(images_dir):\n",
        "  camera = re.search('(stereo|mono_(left|right|rear))', images_dir).group(0)\n",
        "  if camera == 'stereo':\n",
        "    camera_sensor = re.search('(left|centre|right)', images_dir).group(0)\n",
        "    if camera_sensor == 'left':\n",
        "      return 'stereo_wide_left'\n",
        "    elif camera_sensor == 'right':\n",
        "      return 'stereo_wide_right'\n",
        "    elif camera_sensor == 'centre':\n",
        "      return 'stereo_narrow_left'\n",
        "    else:\n",
        "      raise RuntimeError('Unknown camera model for given directory: ' + images_dir)\n",
        "  else:\n",
        "    return camera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njAyVMia6YUz"
      },
      "outputs": [],
      "source": [
        "models_dir = ('/content/drive/MyDrive/sample_small/models')\n",
        "images_dir = ('/content/drive/MyDrive/sample_small/mono_left')\n",
        "__get_model_name(images_dir)\n",
        "load_lut(models_dir,images_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uum5pv_cyu2o"
      },
      "outputs": [],
      "source": [
        "from scipy.ndimage import map_coordinates\n",
        "\n",
        "def undistort(image):\n",
        "  bilinear_lut = load_lut(models_dir,images_dir)\n",
        "  if image.shape[0] * image.shape[1] != bilinear_lut.shape[0]:\n",
        "    raise ValueError('Incorrect image size for camera model')\n",
        "  lut = bilinear_lut[:, 1::-1].T.reshape((2, image.shape[0], image.shape[1]))\n",
        "  if len(image.shape) == 1:\n",
        "      raise ValueError('Undistortion function only works with multi-channel images')\n",
        "  undistorted = np.rollaxis(np.array([map_coordinates(image[:, :, channel], lut, order=1)\n",
        "      for channel in range(0, image.shape[2])]), 0, 3)\n",
        "  return undistorted.astype(image.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Mono Left Images"
      ],
      "metadata": {
        "id": "UkY3R0A3C7Ys"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGLIafgaXkPA"
      },
      "outputs": [],
      "source": [
        "undistort_mono_left = []\n",
        "for img in mono_left_images:\n",
        "  undistort_img = undistort(img)\n",
        "  undistort_mono_left.append(undistort_img)\n",
        "\n",
        "for img, image in enumerate(undistort_mono_left):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.title(str(\"Mono Left Undistorted Images \"+ str(img+1)))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Mono Right Images"
      ],
      "metadata": {
        "id": "oNYXVsw7Erhp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zev7I6y0oE5v"
      },
      "outputs": [],
      "source": [
        "undistort_mono_right = []\n",
        "for img in mono_right_images:\n",
        "  undistort_img = undistort(img)\n",
        "  undistort_mono_right.append(undistort_img)\n",
        "\n",
        "for img, image in enumerate(undistort_mono_right):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.title(str(\"Mono Right Undistorted Images \"+ str(img+1)))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mono Rear Images"
      ],
      "metadata": {
        "id": "1gj9BjoyEwwq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-SWgwFWoaNv"
      },
      "outputs": [],
      "source": [
        "undistort_mono_rear = []\n",
        "for img in mono_rear_images:\n",
        "  undistort_img = undistort(img)\n",
        "  undistort_mono_rear.append(undistort_img)\n",
        "\n",
        "for img, image in enumerate(undistort_mono_rear):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.title(str(\"Mono Rear Undistorted Images \"+ str(img+1)))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOO8gP-QGiYP"
      },
      "source": [
        "###WarpPerspective Function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An array of four points is sent to the \"order points\" function, which outputs the points in the following order: top-left, top-right, bottom-right, top-left.The function achieves this by finding the points with the minimum and maximum sums and differences of their x and y coordinates.\n",
        "\n",
        "To obtain the ordered points of the rectangle in the image, the \"four point transform\" function first invokes the \"order points\" function. It then applies Euclidean distance between the points on the opposing sides of the rectangle to determine the maximum width and height of the resulting transformed image. The transformed coordinates of the rectangle's four corners are represented by a destination array of points.\n",
        "\n",
        "The function then calculates a perspective transformation matrix (M) using the \"cv2.getPerspectiveTransform\" function, which maps the original image to the transformed image.\n",
        "\n",
        "Finally, the \"cv2.warpPerspective\" function is called to apply the transformation to the original image using the perspective transformation matrix.\n",
        "\n",
        "Seperate Points are set for each mono and stereo centre images."
      ],
      "metadata": {
        "id": "eNre985KlWmI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nP-4Hhg_xG1v"
      },
      "outputs": [],
      "source": [
        "def order_points(pts):\n",
        "\n",
        "\trect = np.zeros((4, 2), dtype = \"float32\")\n",
        "\n",
        "\ts = pts.sum(axis = 1)\n",
        "\trect[0] = pts[np.argmin(s)]\n",
        "\trect[2] = pts[np.argmax(s)]\n",
        "\n",
        "\tdiff = np.diff(pts, axis = 1)\n",
        "\trect[1] = pts[np.argmin(diff)]\n",
        "\trect[3] = pts[np.argmax(diff)]\n",
        "\t# return the ordered coordinates\n",
        "\treturn rect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHCDOtHAyF6e"
      },
      "outputs": [],
      "source": [
        "def four_point_transform(image, pts):\n",
        "\n",
        "\trect = order_points(pts)\n",
        "\t(tl, tr, br, bl) = rect\n",
        "\n",
        "\twidthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
        "\twidthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
        "\tmaxWidth = max(int(widthA), int(widthB))\n",
        "\n",
        "\theightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
        "\theightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
        "\tmaxHeight = max(int(heightA), int(heightB))\n",
        "\n",
        "\tdst = np.array([\n",
        "\t\t[0, 0],\n",
        "\t\t[maxWidth - 1, 0],\n",
        "\t\t[maxWidth - 1, maxHeight - 1],\n",
        "\t\t[0, maxHeight - 1]], dtype = \"float32\")\n",
        "\n",
        "\tM = cv2.getPerspectiveTransform(rect, dst)\n",
        "\twarped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
        "\n",
        "\treturn warped\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Mono Left Images"
      ],
      "metadata": {
        "id": "3AwT9NjnFM2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mono-Left Images are rotated to 90 degrees Anticlockwise to present Top-View Generation."
      ],
      "metadata": {
        "id": "mDd6kjMBHl77"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd5w05njfTJP"
      },
      "outputs": [],
      "source": [
        "wrap_mono_left = []\n",
        "pts = np.array(eval((\"[[10,650], [1023,450], [1023,800], [100,800]]\")))\n",
        "for img in undistort_mono_left:\n",
        "  wrap_image = four_point_transform(img,pts)\n",
        "  rotated_img_left = cv2.rotate(wrap_image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "  wrap_mono_left.append(rotated_img_left)\n",
        "\n",
        "for img, image in enumerate(wrap_mono_left):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.title(str(\"Mono Left Wrap Perspective Images \"+ str(img+1)))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mono Right Images"
      ],
      "metadata": {
        "id": "bnDiIFyHFTJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mono-Right Images are rotated to 90 degrees clockwise to present Top-View Generation."
      ],
      "metadata": {
        "id": "0WxDaAxYHZ2w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HFUquwlVQ2r"
      },
      "outputs": [],
      "source": [
        "wrap_mono_right = []\n",
        "pts = np.array(eval((\"[[10,650], [1023,450], [1023,800], [100,800]]\")))\n",
        "for img in undistort_mono_right:\n",
        "  wrap_image = four_point_transform(img,pts)\n",
        "\n",
        "# Rotate the image\n",
        "  rotated_img_right = cv2.rotate(wrap_image, cv2.ROTATE_90_CLOCKWISE)\n",
        "  wrap_mono_right.append(rotated_img_right)\n",
        "\n",
        "for img, image in enumerate(wrap_mono_right):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.title(str(\"Mono Right Wrap Perspective Images \"+ str(img+1)))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Mono Rear Images"
      ],
      "metadata": {
        "id": "Wq6yZyzrFqiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Images are resized so that images can be placed properly in order on the blank image below during stitching. Mono-Rear Images are rotated to 180 degrees to present Top-View Generation."
      ],
      "metadata": {
        "id": "7iZbMymMp7B2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4-Wqm_ZfUvB"
      },
      "outputs": [],
      "source": [
        "wrap_mono_rear = []\n",
        "pts = np.array(eval((\"[[10,650], [1023,450], [1023,800], [100,800]]\")))\n",
        "for img in undistort_mono_rear:\n",
        "  wrap_image = four_point_transform(img,pts)\n",
        "  #resize images\n",
        "  wrap_image = cv2.resize(wrap_image, (1015, 200))\n",
        "  rotated_img_rear = cv2.rotate(wrap_image, cv2.ROTATE_180)\n",
        "  wrap_mono_rear.append(rotated_img_rear)\n",
        "\n",
        "for img, image in enumerate(wrap_mono_rear):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.title(str(\"Mono Rear Wrap Perspective Images \"+ str(img+1)))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stereo Centre Images"
      ],
      "metadata": {
        "id": "D7jHQ7o7FyM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Images are resized so that images can be placed properly in order on the blank image below during stitching."
      ],
      "metadata": {
        "id": "evnfZYeYqK5-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ4dea3BgU2p"
      },
      "outputs": [],
      "source": [
        "wrap_stereo_centre = []\n",
        "pts = np.array(eval((\"[[10,650], [1023,450], [1023,800], [100,800]]\")))\n",
        "for img in stereo_centre_images:\n",
        "  wrap_image = four_point_transform(img,pts)\n",
        "  wrap_image = cv2.resize(wrap_image, (500, 250))\n",
        "  wrap_stereo_centre.append(wrap_image)\n",
        "\n",
        "for img, image in enumerate(wrap_stereo_centre):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.title(str(\"Stereo Centre Wrap Perspective Images \"+ str(img+1)))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt45yQcr_y1Z"
      },
      "source": [
        "###Stitch WarpPerspective Function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A blank image is taken of size (1032,1015,3) on which the mono-left image is pasted on the left, mono-right image is pasted on right side of blank image, stereo centre is pasted on top as the front of the blank image and mono-rear is placed at the bottom on the basis of the shape of each set of images.\n",
        "\n",
        "The front and rear images were resized above in order to place the images correctly on the blank image.\n",
        "\n",
        "Mono-Right,Mono-Left and Mono-Rear Images are rotated to paste correctly on the blank image for generating Top-View\n",
        "\n",
        "An attempt was made to blend the edges to construct the feel of a single image with the help of addWeighted function which is commented below.\n",
        "\n",
        "The front image was improvised in terms of shade with the help of histogram matching the mono rear image(code mentioned above)\n",
        "\n",
        "A blank rectangle in the middle represents the car in the parking slot and the overall image generated represents the generated top-view."
      ],
      "metadata": {
        "id": "AFMLI0TjnykP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asrdIorc5vp6"
      },
      "outputs": [],
      "source": [
        "# merge_img =np.zeros((1280,800,3),dtype = np.uint8)\n",
        "# wrap_mono_left = cv2.resize(rotated_img_left,(600,300))\n",
        "# print(rotated_img_left.shape)\n",
        "# wrap_mono_right = cv2.resize(rotated_img_right,(600,300))\n",
        "# wrap_stereo_centre = cv2.resize(wrap_image,(300,1200))\n",
        "# print(wrap_image.shape)\n",
        "# wrap_mono_rear = cv2.resize(rotated_img_rear,(250,1280))\n",
        "\n",
        "# left_image = (0,0)\n",
        "# rear_image = (90,980)\n",
        "# right_image = (550,0)\n",
        "# front_image = (120,0)\n",
        "\n",
        "# #blend left and right with front\n",
        "# #blend_left = cv2.addWeighted(rotated_img_centre,0.5,cv2.resize(rotated_img_left,(600,300)),0.5,0)\n",
        "# #blend_right = cv2.addWeighted(rotated_img_centre,0.5,cv2.resize(rotated_img_right,(600,300)),0.5,0)\n",
        "\n",
        "# merge_img =np.zeros((1280,800,3),dtype = np.uint8)\n",
        "# merge_img[left_image[1]:left_image[1]+wrap_mono_left.shape[0],left_image[0]:left_image[0]+wrap_mono_left.shape[1]]-wrap_mono_left\n",
        "# merge_img[right_image[1]:right_image[1]+wrap_mono_right.shape[0],right_image[0]:right_image[0]+wrap_mono_right.shape[1]]-wrap_mono_right\n",
        "# merge_img[front_image[1]:front_image[1]+wrap_stereo_centre.shape[0],front_image[0]:front_image[0]+wrap_stereo_centre.shape[1]]-wrap_stereo_centre\n",
        "# merge_img[rear_image[1]:rear_image[1]+wrap_mono_rear.shape[0],rear_image[0]:rear_image[0]+wrap_mono_rear.shape[1]]-wrap_mono_rear\n",
        "\n",
        "# s = cv2.resize(wrap_mono_left,(600,300))\n",
        "# t = cv2.resize(wrap_mono_right,(600,300))\n",
        "\n",
        "# merge_img[0:s.shape[0],0:s.shape[1]] = blend_left\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check sizes of the left,right,rear and centre images to stitch correctly in future code.\n",
        "print(wrap_mono_right[0].shape)\n",
        "print(wrap_mono_left[0].shape)\n",
        "print(wrap_stereo_centre[0].shape)\n",
        "print(wrap_mono_rear[0].shape)"
      ],
      "metadata": {
        "id": "0zYnf2nsmc7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stitched_image=[]\n",
        "for i in range(0,len(wrap_mono_right)):\n",
        "    black_image =np.zeros((1032,1015,3),dtype = np.uint8)\n",
        "    x = (1032 - 500) // 2\n",
        "    y = 0\n",
        "    black_image[y:y+wrap_stereo_centre[i].shape[0],\n",
        "                x:x+wrap_stereo_centre[i].shape[1]] = wrap_stereo_centre[i]\n",
        "    black_image[:, :350, :] = wrap_mono_left[i]\n",
        "    black_image[:, 665:1015, :] = wrap_mono_right[i]\n",
        "    black_image[832:1032, :, :] = wrap_mono_rear[i]\n",
        "    stitched_image.append(black_image)\n",
        "\n",
        "for img, image in enumerate(stitched_image):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.title(str(\"Stereo Centre Wrap Perspective Images \"+ str(img+1)))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "rMOAUsywnMRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task2: Park Slot Detection"
      ],
      "metadata": {
        "id": "kl0f1vI9Vzt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Car Detection"
      ],
      "metadata": {
        "id": "IvagGHyaiuj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Object Detection of the cars Faster R-CNN algorithm from torchvision is implemented.Torchvision is a library to build,train and predict neural networks in Tensorflow.\n",
        "\n",
        "Faster R-CNN is implemented because it consists of only convolution layers resulting in faster training and predicting than other CNN models for object detection.\n",
        "\n",
        "The model is evaluated by using the standard Mean Average Precision at a threshold of 0.90 to detect cars.\n",
        "\n",
        "Prediction is done based on the classes set of the cars and remaining objects as background in the image."
      ],
      "metadata": {
        "id": "z-5HJp_krD65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Define the device for running the model on\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "img_directory = '/content/drive/MyDrive/sample_small/mono_right'\n",
        "img_files = os.listdir(img_directory)\n",
        "process_img = []\n",
        "for img_file in img_files:\n",
        "  img_path = os.path.join(img_directory,img_file)\n",
        "  image = cv2.imread(img_path)\n",
        "  process_img.append(image)\n",
        "# Set the model to evaluation mode\n",
        "\n",
        "final_img = []\n",
        "for i in process_img:\n",
        "  model.eval()\n",
        "  # Define the class labels\n",
        "  classes = ['background', 'car']\n",
        "  # Define the threshold for classifying a detection as a car\n",
        "  threshold = 0.97\n",
        "  image_tensor = torchvision.transforms.functional.to_tensor(i).to(device)\n",
        "  with torch.no_grad():\n",
        "    prediction = model([image_tensor])\n",
        "  # Get the predicted bounding boxes, scores, and labels for the cars\n",
        "  boxes = prediction[0]['boxes'].cpu().numpy()\n",
        "  scores = prediction[0]['scores'].cpu().numpy()\n",
        "  # labels = prediction[0]['labels'].cpu().numpy()\n",
        "  car_boxes = boxes[scores >= threshold]\n",
        "  # car_labels = labels[scores >= threshold]\n",
        "  # Draw the predicted bounding boxes on the image\n",
        "  for box in car_boxes:\n",
        "      x1, y1, x2, y2 = box.astype(int)\n",
        "      rect = cv2.rectangle(i, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "  final_img.append(i)\n",
        "\n"
      ],
      "metadata": {
        "id": "VMUxSNRHv70U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for img, image in enumerate(final_img):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(image)\n",
        "    plt.title(\"Car Detection Mono Right Images: \"+ str(img+1))\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pag7K915hBaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detect Parking Lines"
      ],
      "metadata": {
        "id": "2Z8H9XvVWs99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Detect Parking Lines following steps were followed which will be explained in detail during the code implementation:\n",
        "\n",
        "1) Take Images which Detect Cars in the Above Step\n",
        "\n",
        "2) Apply Gaussian Blur\n",
        "\n",
        "3) Apply Canny Edge Detection\n",
        "\n",
        "4) Finding Region of Interest\n",
        "\n",
        "5) Detection of Parking Lines\n",
        "\n"
      ],
      "metadata": {
        "id": "F-yPVUESumEQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Upl2UFVJzcx"
      },
      "source": [
        "####Apply Gaussian Blur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYOxm-yBJnhn"
      },
      "outputs": [],
      "source": [
        "kernel_size = (15,15)\n",
        "\n",
        "blur_images = []\n",
        "# blur all of the images with opencv's gaussian blur function and populate the blur_images list with the results\n",
        "for i in final_img:\n",
        "    resultimage = cv2.GaussianBlur(i,kernel_size,0)\n",
        "    blur_images.append(resultimage)\n",
        "\n",
        "#display blurred images\n",
        "for bi, biimage in enumerate(blur_images):\n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.title(str(\"Gaussian Blur Image of Mono Right Images \"))\n",
        "  plt.imshow(biimage, cmap=\"gray\")\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Apply Canny Edge Detection"
      ],
      "metadata": {
        "id": "dUaB93DEpeZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "canny_low_threshold = 70\n",
        "canny_high_threshold = 200\n",
        "\n",
        "# apply edge detection to the blur_images list of images\n",
        "canny_images = []\n",
        "for i in blur_images:\n",
        "    edge = cv2.Canny(i, canny_low_threshold, canny_high_threshold)\n",
        "    canny_images.append(edge)\n",
        "#display edge images\n",
        "for ci, ciimage in enumerate(canny_images):\n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.title(str(\"Canny Edge Detection of Mono Right Images\"))\n",
        "  plt.imshow(ciimage, cmap=\"gray\")\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "GDVgk_O3pfIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finding Region of Interest"
      ],
      "metadata": {
        "id": "ThGHUod2qjQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def region_of_interest(img, vertices):\n",
        "    \"\"\"\n",
        "    Applies an image mask.\n",
        "\n",
        "    Only keeps the region of the image defined by the polygon defined by \"vertices\".\n",
        "    The rest of the image is set to black.\n",
        "    \"\"\"\n",
        "    #define a blank mask\n",
        "    mask = np.zeros_like(img)\n",
        "\n",
        "    #define a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
        "    if len(img.shape) > 2:\n",
        "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
        "        ignore_mask_color = (0,255,255) * channel_count\n",
        "    else:\n",
        "        ignore_mask_color = 255\n",
        "\n",
        "    #filling pixels inside the polygon defined by \"vertices\" with the fill color\n",
        "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
        "\n",
        "    #returning the image only where mask pixels are nonzero\n",
        "    masked_image = cv2.bitwise_and(img, mask)\n",
        "    return masked_image"
      ],
      "metadata": {
        "id": "HBzpiaDKqi5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_images(raw_images, canny_images, region):\n",
        "  ysize = raw_images[0].shape[0]\n",
        "  xsize = raw_images[0].shape[1]\n",
        "  # define region of interest. The region of interest should include the road area and ignore the background\n",
        "  # the coordinates will need to be changed, depending on the image set used, as the field of view will be different\n",
        "  # define the region as a numpy array, of type np.int32\n",
        "\n",
        "  #region = np.array() # put your code here\n",
        "  region = np.array([[(0,750),(10,600),(1023,500),(1023,800)]], dtype= np.int32)\n",
        "  # apply the mask to the Canny edge images\n",
        "  masked_images = [region_of_interest(img,[region]) for img in canny_images]\n",
        "  return masked_images\n",
        "\n"
      ],
      "metadata": {
        "id": "-8-G6_nDwC2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "region = np.array([[(0,750),(10,600),(1023,500),(1023,800)]], dtype= np.int32)\n",
        "\n",
        "masked_img = masked_images(mono_right_images, canny_images,region)\n",
        "for img in masked_img:\n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.title(str(\"Region of Interest of Mono Right Images\"))\n",
        "  plt.imshow(img, cmap=\"gray\")\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "3znadIX8q7y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Detection of Parking Lines"
      ],
      "metadata": {
        "id": "b60ad33qX_M6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize the canny image to match the size of the actual image\n",
        "detect_parking = []\n",
        "for i, image in enumerate(masked_img):\n",
        "  canny_image_resized = cv2.resize(image, (final_img[i].shape[1], final_img[i].shape[0]))\n",
        "\n",
        "# Convert the canny image to a color image with the same number of channels as the actual image\n",
        "  canny_image_color = cv2.cvtColor(canny_image_resized, cv2.COLOR_GRAY2RGB)\n",
        "  canny_image_color[canny_image_resized > 0] = [0, 255, 0]\n",
        "# Set a blending factor for the canny image (0.5 = 50-50 blend)\n",
        "  alpha = 0.6\n",
        "  green = np.array([0, 255, 0], dtype=np.uint8)\n",
        "# Overlay the canny image on the actual image using alpha blending\n",
        "  result = cv2.addWeighted(final_img[i], alpha, canny_image_color, 1, 0)\n",
        "  detect_parking.append(result)\n",
        "\n",
        "for img in detect_parking:\n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.title(str(\"Parking Lines Detection of Mono Right Images\"))\n",
        "  plt.imshow(img, cmap=\"gray\")\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "hl52NCRst2rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parking Slot Segmentation"
      ],
      "metadata": {
        "id": "saZx4lMVMRuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tried following the assignments and Jupyter Notebooks provided in the course module but failed to come up with correct implementation of the code.\n",
        "\n",
        "Possible way to segment the parking slot apart from Neural Network Implementation could be applying Morphological Operations on the image.\n",
        "\n",
        "FindContours,Draw Contours and then FillPoly function could be applied to segment the parking slot region."
      ],
      "metadata": {
        "id": "SkPia-UXMXd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result Image"
      ],
      "metadata": {
        "id": "2_6qy_gHxBVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A final output image is created by resizing the Top View and Park Slot Detection and rotated"
      ],
      "metadata": {
        "id": "PCrxRYLt1bBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_image(left,right):\n",
        "    result_image = []\n",
        "    final_output = np.zeros((800,1280,3), dtype=np.uint8)\n",
        "\n",
        "    # Resize the front, right, left, and rear images to specific sizes\n",
        "    right = cv2.rotate(right, cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "    left = cv2.resize(left,(640,800))\n",
        "    right = cv2.resize(right,(640,800))\n",
        "\n",
        "    # Define the locations where the resized images will be placed in the final merged image\n",
        "    right_location = (640,0)\n",
        "    left_location = (0,0)\n",
        "\n",
        "    # Merge the four images into a single image by placing them in their respective locations\n",
        "    final_output[left_location[1]:left_location[1]+left.shape[0],left_location[0]:left_location[0]+left.shape[1]]=left\n",
        "    final_output[right_location[1]:right_location[1]+right.shape[0],right_location[0]:right_location[0]+right.shape[1]]=right\n",
        "    result_image.append(final_output)\n",
        "\n",
        "    return final_output"
      ],
      "metadata": {
        "id": "zs0C7a4b1CPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_img = final_image(stitched_image[0],detect_parking[0])\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.title(str(\"Resultant Output\"))\n",
        "plt.imshow(final_img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J-DI05UO_a4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_images1 = []\n",
        "for i,img in enumerate(stitched_image):\n",
        "  final_img = final_image(stitched_image[i],detect_parking[i])\n",
        "  final_images1.append(final_img)\n",
        "\n",
        "for image in final_images1:\n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.title(str(\"Resultant Output\"))\n",
        "  plt.imshow(image)\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "FsWxn_Cf4mov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Limitations\n"
      ],
      "metadata": {
        "id": "zekVRCbr6Nz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Parking Slot Segmentation does not work properly.\n",
        "\n",
        "2) There is potential to improve the Top-View Generated in the code above.\n",
        "\n",
        "3) Parking Lines not implemented correctly for 2-3 images\n",
        "\n",
        "4) Even after the front image in Top-View Generation is improvised in terms of quality by histogram matching the image is not a perfect match to left,right and rear images.\n"
      ],
      "metadata": {
        "id": "Kw4TB1SG6Qad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Potential Improvements"
      ],
      "metadata": {
        "id": "3j_p82xP6UVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) A different model could have been chosen and tested for better object detection results such as YOLO or SSD(Single Shot Detector).\n",
        "\n",
        "2) 2 to 3 images do not detect parking lines correctly which can be improvised by using Hough Tranform or Template Matching Technique. Template Matching detects parking lines by matching a template image of a parking line to the corresponding region in the larger image.\n",
        "\n",
        "3) The Top View generated could be improvised by changing the WarpPerspective points so that cars are not detected and fine road with parking lane is generated.\n",
        "\n",
        "4) The Top View generated could be improvised by adding addWeighted function code correctly to merge the left with front and rear and right with front and rear correctly.\n",
        "\n",
        "5) The Parking Slot could be segmented by re-training a neural network model and segmenting the region where parking lines are detected.\n",
        "\n",
        "6) Top-View Generation could be improved by stitching more number of images than 4 (8/12 etc)are performed in the code above.\n",
        "\n",
        "7) The quality of Front Image in Top-View Generation could be further improvised by applying images brightness and contrast functions to match with the mono-left,mono-right and mono-rear images."
      ],
      "metadata": {
        "id": "6koP8n6i6y7A"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wIXqYD6BXMg"
      },
      "source": [
        "## References\n",
        "\n",
        "1) **Detecting Parking Lines:** Embedded Image Processing Assignment 5\n",
        "\n",
        "2) https://github.com/ori-mrg/robotcar-dataset-sdk/blob/master/python/\n",
        "\n",
        "3) **Demosaic Function:** Embedded Image Processing Assignment 1\n",
        "\n",
        "4) **WarpPerspective Function:** https://pyimagesearch.com/2014/08/25/4-point-opencv-getperspective-transform-example/\n",
        "\n",
        "5) **UnDistortion Function:** Embedded Image Processing Assignment 3\n",
        "\n",
        "6) **Improving Colour Shade of Stereo Centre Image:** Intensity transforms, histogram operations.ipynb provided under Embedded Image Processing Laerning Materials-> Jupyter Notebooks on Blackboard.\n",
        "\n",
        "7) **Car Object Detection:** https://pytorch.org/vision/master/models/faster_rcnn.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}